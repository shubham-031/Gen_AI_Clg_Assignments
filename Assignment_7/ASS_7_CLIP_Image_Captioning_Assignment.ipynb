{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f79041",
   "metadata": {},
   "source": [
    "# Image Captioning using CLIP (Multimodal System)\n",
    "\n",
    "## Aim\n",
    "To implement a basic multimodal system for image captioning using CLIP.\n",
    "\n",
    "## Objective\n",
    "To understand how images and text can be processed together using a pre-trained multimodal model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1bb5bb",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "**Image Captioning** is a multimodal task where a system generates a textual description for a given image.\n",
    "\n",
    "- Uses both **Computer Vision** and **Natural Language Processing**\n",
    "- CLIP is a popular multimodal model developed by OpenAI\n",
    "- CLIP learns image–text relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ac0ae",
   "metadata": {},
   "source": [
    "## Model Used\n",
    "\n",
    "- **CLIP (Contrastive Language–Image Pre-training)**\n",
    "- Pre-trained on image–text pairs\n",
    "- Maps images and text into a common embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3c238",
   "metadata": {},
   "source": [
    "## Working Principle (Pipeline)\n",
    "\n",
    "1. Input image is encoded using image encoder\n",
    "2. Candidate text captions are encoded using text encoder\n",
    "3. Similarity between image and text embeddings is computed\n",
    "4. Caption with highest similarity is selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88541e11",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de54527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install torch, torchvision==0.17.1, torchvision==0.17.2, torchvision==0.18.0, torchvision==0.18.1, torchvision==0.19.0, torchvision==0.19.1, torchvision==0.20.0, torchvision==0.20.1, torchvision==0.21.0, torchvision==0.22.0, torchvision==0.22.1, torchvision==0.23.0, torchvision==0.24.0, torchvision==0.24.1 and torchvision==0.25.0 because these package versions have conflicting dependencies.\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision ftfy regex tqdm pillow clip-by-openai --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b492f",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e919c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da81591d",
   "metadata": {},
   "source": [
    "## Step 3: Load CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf252ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:50<00:00, 6.97MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "print(\"CLIP model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf7e1d",
   "metadata": {},
   "source": [
    "## Step 4: Load Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1f5905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.6682, -0.8288, -0.7704,  ...,  1.9157,  1.9157,  1.9157],\n",
       "          [-0.6682, -0.8288, -0.7704,  ...,  1.9157,  1.9157,  1.9157],\n",
       "          [-0.6390, -0.8142, -0.7558,  ...,  1.9157,  1.9157,  1.9157],\n",
       "          ...,\n",
       "          [ 1.7844,  1.7844,  1.7844,  ...,  1.7552,  1.7552,  1.7552],\n",
       "          [ 1.7844,  1.7844,  1.7844,  ...,  1.7552,  1.7552,  1.7552],\n",
       "          [ 1.7844,  1.7844,  1.7844,  ...,  1.7698,  1.7552,  1.7552]],\n",
       "\n",
       "         [[ 0.6041,  0.5741,  0.6642,  ...,  1.9698,  1.9698,  1.9698],\n",
       "          [ 0.6191,  0.5741,  0.6642,  ...,  1.9698,  1.9698,  1.9698],\n",
       "          [ 0.6341,  0.5891,  0.6792,  ...,  1.9698,  1.9698,  1.9698],\n",
       "          ...,\n",
       "          [ 0.5741,  0.5741,  0.5741,  ...,  0.6792,  0.6642,  0.6642],\n",
       "          [ 0.5441,  0.5441,  0.5441,  ...,  0.6041,  0.5891,  0.5891],\n",
       "          [ 0.5291,  0.5291,  0.5291,  ...,  0.5591,  0.5441,  0.5441]],\n",
       "\n",
       "         [[ 0.5817,  0.6244,  0.7381,  ...,  1.6766,  1.6766,  1.6766],\n",
       "          [ 0.5959,  0.6386,  0.7381,  ...,  1.6766,  1.6766,  1.6766],\n",
       "          [ 0.6101,  0.6528,  0.7523,  ...,  1.6766,  1.6766,  1.6766],\n",
       "          ...,\n",
       "          [ 0.5675,  0.5675,  0.5675,  ...,  0.5248,  0.5248,  0.5248],\n",
       "          [ 0.4821,  0.4821,  0.4821,  ...,  0.5248,  0.5106,  0.5106],\n",
       "          [ 0.4253,  0.4253,  0.4253,  ...,  0.5248,  0.5106,  0.5106]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use any sample image available on your system\n",
    "image = preprocess(Image.open(\"download.jpeg\")).unsqueeze(0).to(device)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a63688",
   "metadata": {},
   "source": [
    "## Step 5: Define Candidate Captions\n",
    "\n",
    "Since CLIP is not a generative model, we select the best caption from predefined text options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63992f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [\n",
    "    \"a photo of a cat\",\n",
    "    \"a photo of a dog\",\n",
    "    \"a photo of a person\",\n",
    "    \"a photo of a car\"\n",
    "]\n",
    "\n",
    "text_tokens = clip.tokenize(captions).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13764eb",
   "metadata": {},
   "source": [
    "## Step 6: Image Caption Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758c3f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: a photo of a person\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "best_caption = captions[similarity.argmax().item()]\n",
    "print(\"Predicted Caption:\", best_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a048966",
   "metadata": {},
   "source": [
    "## Observations (Exam Ready Points)\n",
    "\n",
    "- CLIP successfully matches image with relevant text\n",
    "- Caption selection is based on similarity score\n",
    "- Demonstrates multimodal learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578d0a3",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "- Image captioning systems\n",
    "- Image search and retrieval\n",
    "- Accessibility tools for visually impaired\n",
    "- Multimodal AI systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0736e584",
   "metadata": {},
   "source": [
    "## Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- No training required\n",
    "- Works for both image and text\n",
    "\n",
    "**Limitations:**\n",
    "- Cannot generate new captions\n",
    "- Depends on predefined text options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda35696",
   "metadata": {},
   "source": [
    "## Conclusion (One-Line Exam Answer)\n",
    "\n",
    "CLIP enables image captioning by matching images with the most relevant text using a shared multimodal embedding space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
