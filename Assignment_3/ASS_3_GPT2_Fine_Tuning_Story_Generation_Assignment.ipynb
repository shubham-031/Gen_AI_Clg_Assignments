{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1781fcdf",
   "metadata": {},
   "source": [
    "# Fine-Tuning GPT-2 for Creative Story Generation\n",
    "\n",
    "\n",
    "## Aim\n",
    "To fine-tune a pre-trained GPT-2 model for creative story generation.\n",
    "\n",
    "## Objective\n",
    "To understand how a large language model can be adapted to a specific task using fine-tuning on custom data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d58efd",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "**GPT-2 (Generative Pre-trained Transformer-2)** is a transformer-based language model developed by OpenAI.\n",
    "\n",
    "- It is pre-trained on large text data\n",
    "- Fine-tuning helps adapt it to a specific task\n",
    "- Here, GPT-2 is fine-tuned for **creative story generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a06bc",
   "metadata": {},
   "source": [
    "## Why Fine-Tuning is Needed?\n",
    "\n",
    "- Pre-trained models give general responses\n",
    "- Fine-tuning improves task-specific creativity\n",
    "- Helps generate domain-specific stories\n",
    "- Improves style and consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d6b48",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eba6de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0c9e38",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb0c191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af498c5",
   "metadata": {},
   "source": [
    "## Step 3: Create Story Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab532f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample story dataset (exam-friendly)\n",
    "stories = [\n",
    "    \"Once upon a time, a young robot dreamed of becoming human.\",\n",
    "    \"In a small village, there lived a boy who could talk to animals.\",\n",
    "    \"A mysterious door appeared in the forest every full moon.\",\n",
    "    \"The future city was powered entirely by artificial intelligence.\"\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": stories})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3a24a",
   "metadata": {},
   "source": [
    "## Step 4: Load Pre-trained GPT-2 Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a220cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 148/148 [00:00<00:00, 321.59it/s, Materializing param=transformer.wte.weight]             \n",
      "\u001b[1mGPT2LMHeadModel LOAD REPORT\u001b[0m from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# GPT-2 does not have pad token by default\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0325bcb",
   "metadata": {},
   "source": [
    "## Step 5: Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "821da74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4/4 [00:00<00:00, 85.25 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=64)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80776c",
   "metadata": {},
   "source": [
    "## Step 6: Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06fe1f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-story-model\",\n",
    "    num_train_epochs=1,          # low for exam demo\n",
    "    per_device_train_batch_size=2,\n",
    "    logging_steps=5,\n",
    "    learning_rate=5e-5,\n",
    "    report_to=\"none\"             # avoids warnings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9909cbc7",
   "metadata": {},
   "source": [
    "## Step 7: Fine-Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54137d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffffc172",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False   # GPT-2 is NOT masked LM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca0ba55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=4.046947479248047, metrics={'train_runtime': 8.4123, 'train_samples_per_second': 0.475, 'train_steps_per_second': 0.238, 'total_flos': 130646016000.0, 'train_loss': 4.046947479248047, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b6d14",
   "metadata": {},
   "source": [
    "## Step 8: Generate Creative Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a170c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there of, or upon an end of an unending sequence of, we are not the first and we are not the last.\n",
      "\n",
      "\n",
      "The fourth of the last is in order of or on our last, which is\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f64abd3",
   "metadata": {},
   "source": [
    "## Observations (Exam Ready Points)\n",
    "\n",
    "- Model generates creative stories\n",
    "- Fine-tuned model follows training style\n",
    "- Output is more domain-specific than base GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d3ce4",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "- Story and script writing\n",
    "- Game narrative generation\n",
    "- Content creation\n",
    "- Creative writing assistants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5bc46f",
   "metadata": {},
   "source": [
    "## Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- Improves creativity\n",
    "- Task-specific output\n",
    "\n",
    "**Limitations:**\n",
    "- Requires training data\n",
    "- Computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc902cfc",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "Fine-tuning GPT-2 adapts a pre-trained language model to generate creative stories by learning patterns from custom story data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
